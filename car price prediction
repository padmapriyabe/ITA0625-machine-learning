import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load the dataset (you can replace this with your dataset)
# For this example, I'm using the Car Prices dataset from Kaggle: https://www.kaggle.com/hellbuoy/car-price-prediction
# Download and place the dataset in the same directory as this script.

# Reading the dataset
data = pd.read_csv("car_data.csv")

# Display the first few rows of the dataset
print(data.head())

# Data preprocessing and feature engineering (you may need to customize this based on your dataset)
# For simplicity, I'll use only a few numeric features in this example

# Selecting numeric features
numeric_features = data.select_dtypes(include=[np.number])

# Handling missing values (you may need to customize this based on your dataset)
numeric_features = numeric_features.fillna(0)

# Selecting independent variables (features) and dependent variable (target)
X = numeric_features.drop('price', axis=1)
y = numeric_features['price']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("R-squared:", r2)

# Visualize the predicted vs. actual prices
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Car Price Prediction")
plt.show()
